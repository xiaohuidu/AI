原文链接：https://blog.csdn.net/chumingqian/article/details/131315745

本文探讨了监督学习、半监督学习、弱监督学习和自监督学习的概念。监督学习依赖于带标签的数据，而半监督学习利用少量标签数据和大量未标记数据进行训练。自监督学习通过数据的内在结构创建人工标签，无监督学习则寻找数据中的模式和结构，如聚类。各种学习方法各有优缺点，适用于不同场景和任务

## 1. 有监督

[有监督学习](https://so.csdn.net/so/search?q=%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)的特点，是数据集通常带有人工标签的数据集。

![enter image description here](https://github.com/xiaohuidu/AI/blob/master/images/196.png)


**1.1 定义**

监督学习提供了一组输入输出对，这样我们就可以学习一个将输入映射到正确输出的中间系统。

监督学习的一个简单示例是根据图像数据集及其相应类别（我们将其称为标签）确定图像的类别（即，狗/猫等）。

对于给定的输入标签对，当前流行的方法是直接训练深度神经网络（即卷积神经网络）以从给定图像输出标签预测，计算预测与实际正确值之间的可微损失答案，并通过网络反向传播来更新权重以优化预测。

总的来说，监督学习是最直接的学习方法，因为它假设给定每个图像的标签，这简化了学习过程，因为网络更容易学习。

有监督学习的局限性，数据的标注过程是昂贵的。
虽然监督学习假设要训练任务的整个数据集对于每个输入都有相应的标签，但现实可能并不总是这样。

标记是一项劳动密集型处理任务，输入数据通常是不成对的。

**1.2　半监督学习**

半监督学习旨在解决这个问题：我们如何使用一小组输入输出对和另一组仅输入来优化我们正在解决的任务的模型？

回顾图像分类任务，图像和图像标签现在仅部分存在于数据集中。是否可以在没有任何标签的情况下继续使用数据？

简短的回答是，是的。事实上，有一个称为伪标签的简单技巧可以做到这一点。首先，我们使用具有正确标签的图像来训练分类模型。然后我们使用这个分类模型来标记未标记的图像。然后将带有来自模型的高置信度标签的图像添加到模型中，并将其预测标签作为伪标签用于继续训练。我们迭代这个过程，直到所有数据都被用于最佳分类模型。

![enter image description here](https://github.com/xiaohuidu/AI/blob/master/images/197.png)

**半监督学习的局限性**：当然，这种方法虽然看似聪明，但很容易出错。如果标记数据的数量非常有限，则模型很可能对训练数据过度拟合，并在早期给出错误的伪标签，导致整个模型完全错误。因此，确定将输入伪标签对纳入训练的置信度阈值也非常重要。

为了避免模型在早期阶段过度拟合，还可以采用数据增强技术来增加训练规模并创建更广泛的数据分布。如果有兴趣，您还可以参考我关于混合作为图像分类任务最主要的增强策略之一的文章。

半监督和弱监督，都属于有监督的类型；
半监督或弱监督混淆：（半监督和弱监督）指的是在数据集中部分一些例子X没有标签，但是数据的人工标签是存在的。

**1.3　弱监督学习**

## 2. 自监督self-supervised learning

自监督，是指从输入数据中构建出标签或者监督信号；

它利用数据本身的固有结构或信息来创建训练的标签。自我监督学习不是依靠人类标记的数据，而是从输入数据中创建人工标签或监督信号。这些人工标签是使用特定的先验任务生成的，旨在捕捉数据中的有用信息或关系。

自监督学习是从数据本身找标签来进行有监督学习。
在自我监督学习(SSL) 中，您使用自己的输入X
（或修改，例如作物或应用数据增强）作为监督。

**2.1关键思想**

自监督学习的关键思想是设计一个先验任务，要求模型预测输入数据中缺失或损坏的部分。通过训练模型来解决这个任务，它学会了捕捉对下游任务有用的有意义的特征或表示。

自监督学习中的先验任务的例子包括预测图像的缺失部分（例如，绘画），预测shuffle 后的图像补丁的顺序，或预测视频中的未来帧。

SSL 主要用于预训练和表征学习。因此，在以后的下游任务中引导一些模型。

自监督学习的代表是语言模型，自监督不需要额外提供label，只需要从数据本身进行构造。

**2.2 　对比学习（正负对）**

正对:对相同的图像进行增强，并将它们标记为正对;

负对: 将不同的图像标记为负对，并尝试将负对的学习特征推开，同时将正特征拖近;

这使得网络能够学习对相似类别的图像进行分组，这进一步使得原本需要固定标签来学习的分类和分割等任务在没有给定基本事实的情况下变得可能。

![enter image description here](https://github.com/xiaohuidu/AI/blob/master/images/198.png)


 **2.3 自我预测方法**

给定一个数据样本，我们尝试填充故意缺失的部分。例如，预测句子中的掩码词或为灰度图像着色。

![enter image description here](https://github.com/xiaohuidu/AI/blob/master/images/199.png)


**2.4 自监督学习的模型**

Barlow Twins 架构于 2021 年 6 月首次在论文“Barlow Twins: Self-Supervised Learning via Redundancy Reduction”中提出。Barlow twins 的名字来源于神经科学家 H. Barlow，他在 1961 年的工作启发了这种架构。它是一种最先进的 ssl 方法，它依赖于直观且易于实现的想法。

最近的自监督学习研究的目标是学习对输入样本失真不变的嵌入。这是通过对输入样本应用扩充并尽可能“驱动”它们的表示来实现的。但一个反复出现的问题是存在微不足道的常数解。当网络依赖于这种微不足道的解决方案时，就不会发生有意义的学习。

已经提出了许多自监督模型和架构（BYOL、SimCLR、DeepCluster、SIMSIAM、SELA、SwAV）。Barlow twins 在几个计算机视觉下游任务上的表现优于或与大多数方法相当。

与其他自监督方法类似，Barlow twins 旨在学习对输入失真不变的表示（嵌入）。它通过 3 个步骤实现。

将两组不同的增强应用于同一输入样本 X，导致同一图像的两个扭曲视图 (Y^a, Y^b)。
扭曲的视图被送入完全相同的编码器，在下面的图 1 中表示为 f。我们将这两个输出分别称为 Za、Zb。
计算两个嵌入向量的互相关矩阵 M。应用损失，计算矩阵 M 与单位矩阵 I 的距离。

![enter image description here](https://github.com/xiaohuidu/AI/blob/master/images/1910.png)

一个重要的细节是编码器 f 实际上是由一个编码器网络和一个“投影仪”网络组成的。我们将编码器的输出称为“表示”，将投影仪的输出称为嵌入。编码器执行特征提取，而投影仪设计嵌入的高维空间。学习到的表示用于在下游任务上训练分类器，而嵌入被馈送到损失函数中以训练模型。

由于失真表示不变是我们的主要目标，因此编码器 f 在同一样本的失真视图上生成的嵌入应该几乎相同。如果它们相同，则互相关矩阵 M 将等于单位矩阵。通过最小化我们的损失函数，我们尽可能接近它们的嵌入和表示。此外，通过将 M 驱动到单位矩阵 I，我们将彼此的嵌入维度去相关。

**2.5　自监督学习的优势**

无需人工标注
高质量的标记数据很昂贵，而且通常很稀缺。自监督训练方法不依赖人工标签来学习有用的特征。这使我们能够利用野外可用的大量数据。
像 BERT 或 GPT-3 这样的大型语言模型之所以出奇地有效，不仅仅是因为它们的体积庞大。这些模型是在 huuuge 文本语料库上训练的。互联网是他们的游乐场，他们可以自由地从书籍、维基百科和常见互联网站点中的所有文本中学习。

另一方面，视觉模型可以收集 Instagram 照片和 youtube 视频进行训练。这使他们能够建立更多的常识

模型可扩展
上述观点的直接结果是，使用 SSL 方法训练的模型可以而且确实可以扩展到前所未有的规模。其他技术因素，如更好的硬件，也有助于此。但是由于较大的模型有更多的参数需要优化，因此在训练过程中需要更多的数据。SSL 是满足庞大模型数据需求的唯一途径。



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExMDYxMjQ4NjEsLTc0NjA1MzY0MCwtMT
g5ODYzMTU1Ml19
-->